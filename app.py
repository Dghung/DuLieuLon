import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.functions import col, regexp_replace, trim
import os
import sys

# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---
# Gi√∫p Streamlit t√¨m th·∫•y Python & PySpark trong m√¥i tr∆∞·ªùng ·∫£o
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# --- 1. KH·ªûI T·∫†O SPARK & LOAD M√î H√åNH T·ª™ HDFS ---
@st.cache_resource
def load_spark_model():
    # Kh·ªüi t·∫°o Spark Session
    # master("local[*]"): D√πng t·∫•t c·∫£ nh√¢n CPU c·ªßa m√°y ƒë·ªÉ ch·∫°y cho nhanh
    spark = SparkSession.builder \
        .appName("FakeNewsApp") \
        .master("local[*]") \
        .getOrCreate()
    
    # Load m√¥ h√¨nh t·ª´ HDFS (ƒê√£ s·ª≠a ƒë∆∞·ªùng d·∫´n th√†nh localhost)
    model_path = "hdfs://localhost:9000/user/hdoop/fake_news_model_final"
    model = PipelineModel.load(model_path)
    return spark, model

# --- 2. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (UI) ---
st.set_page_config(page_title="Fake News Detector", page_icon="üïµÔ∏è", layout="centered")

st.title("üïµÔ∏è Ph√°t hi·ªán Tin gi·∫£ (Fake News)")
st.caption("H·ªá th·ªëng s·ª≠ d·ª•ng **Apache Spark** & **Logistic Regression**")

# T·∫£i m√¥ h√¨nh (Ch·ªâ ch·∫°y 1 l·∫ßn ƒë·∫ßu ti√™n)
try:
    with st.spinner('ƒêang k·∫øt n·ªëi HDFS v√† t·∫£i m√¥ h√¨nh...'):
        spark, model = load_spark_model()
    st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!", icon="üü¢")
except Exception as e:
    st.error(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi Spark/HDFS: {e}")
    st.info("G·ª£i √Ω: H√£y ki·ªÉm tra xem Hadoop ƒë√£ b·∫≠t ch∆∞a (l·ªánh `jps`)?")
    st.stop()

# Khung nh·∫≠p li·ªáu
user_input = st.text_area("Nh·∫≠p n·ªôi dung tin t·ª©c ti·∫øng Anh:", height=200, 
                          placeholder="Paste b√†i b√°o v√†o ƒë√¢y (V√≠ d·ª•: WASHINGTON (Reuters) - ...)")

# --- 3. X·ª¨ L√ù D·ª∞ ƒêO√ÅN ---
if st.button("üîç Ki·ªÉm tra ƒë·ªô tin c·∫≠y", type="primary"):
    if not user_input.strip():
        st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung ƒë·ªÉ ki·ªÉm tra.")
    else:
        with st.spinner('AI ƒëang ph√¢n t√≠ch vƒÉn phong v√† t·ª´ v·ª±ng...'):
            # A. T·∫†O DATAFRAME T·ª™ INPUT
            df_test = spark.createDataFrame([(user_input,)], ["text"])
            
            # B. TI·ªÄN X·ª¨ L√ù TH·ª¶ C√îNG (B·∫Øt bu·ªôc ph·∫£i c√≥ b∆∞·ªõc n√†y!)
            # L√Ω do: Pipeline ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu s·∫°ch. Ta ph·∫£i x√≥a r√°c (Dateline) tr∆∞·ªõc.
            robust_pattern = r"^.*?\s*\(.*?\)\s*-\s*" # M·∫´u x√≥a: "WASHINGTON (Reuters) - "
            
            df_clean = df_test.withColumn("text", regexp_replace(col("text"), robust_pattern, ""))
            df_clean = df_clean.withColumn("text", trim(col("text")))
            
            # C. D·ª∞ ƒêO√ÅN (Ch·∫°y qua Pipeline: Tokenizer -> Remover -> TF -> IDF -> Model)
            prediction = model.transform(df_clean)
            
            # D. L·∫§Y K·∫æT QU·∫¢
            result = prediction.select("prediction", "probability").collect()[0]
            is_fake = (result['prediction'] == 0.0) # 0.0 l√† Fake (theo nh√£n c·ªßa t·∫≠p Fake.csv)
            probs = result['probability']
            
            # E. HI·ªÇN TH·ªä K·∫æT QU·∫¢
            st.divider()
            
            if is_fake:
                # Tr∆∞·ªùng h·ª£p Tin Gi·∫£
                confidence = probs[0] * 100
                st.error(f"üö® K·∫æT QU·∫¢: TIN GI·∫¢ (FAKE NEWS)")
                st.metric(label="ƒê·ªô tin c·∫≠y c·ªßa d·ª± ƒëo√°n", value=f"{confidence:.2f}%")
                st.warning("C·∫£nh b√°o: B√†i vi·∫øt n√†y c√≥ vƒÉn phong gi·∫≠t g√¢n, thi·∫øu c·∫•u tr√∫c chu·∫©n c·ªßa b√°o ch√≠.")
            else:
                # Tr∆∞·ªùng h·ª£p Tin Th·∫≠t
                confidence = probs[1] * 100
                st.success(f"‚úÖ K·∫æT QU·∫¢: TIN TH·∫¨T (REAL NEWS)")
                st.metric(label="ƒê·ªô tin c·∫≠y c·ªßa d·ª± ƒëo√°n", value=f"{confidence:.2f}%")
                st.info("B√†i vi·∫øt n√†y c√≥ c·∫•u tr√∫c v√† t·ª´ v·ª±ng ph√π h·ª£p v·ªõi tin t·ª©c ch√≠nh th·ªëng.")

# --- Footer ---
st.markdown("---")
st.markdown("*Demo Project - Big Data with PySpark*")